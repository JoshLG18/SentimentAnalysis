{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d7f31c",
   "metadata": {},
   "source": [
    "## Transformer - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fac25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchtext\n",
    "#%pip install torchdata\n",
    "#%pip install torchdata==0.7.1\n",
    "#%pip install portalocker\n",
    "#%pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34935a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import math\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "\n",
    "# set device\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# set seed\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)                           # Python random\n",
    "    np.random.seed(seed)                        # NumPy\n",
    "    torch.manual_seed(seed)                     # PyTorch CPU\n",
    "    torch.cuda.manual_seed_all(seed)            # PyTorch GPU (if using)\n",
    "    torch.backends.cudnn.deterministic = True   # For reproducibility\n",
    "    torch.backends.cudnn.benchmark = False      # Disable auto-tuning (slower but stable)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be72a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe coverage: 100.00%\n",
      "Label distribution in training set:\n",
      "Counter({0: 12500, 1: 12500})\n"
     ]
    }
   ],
   "source": [
    "# ========== Hyperparameters ==========\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "MAX_LEN = 300\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "# ========== Load IMDB Dataset ==========\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Tokenizer and GloVe ==========\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "glove = GloVe(name=\"6B\", dim=EMBEDDING_DIM)\n",
    "\n",
    "def yield_glove_tokens(data_iter):\n",
    "    for example in data_iter:\n",
    "        tokens = tokenizer(example[\"text\"])\n",
    "        yield [token for token in tokens if token in glove.stoi]\n",
    "\n",
    "# ========== Build Vocab (only GloVe tokens) ==========\n",
    "vocab = build_vocab_from_iterator(yield_glove_tokens(train_data), specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# ========== GloVe Coverage Check ==========\n",
    "known = sum(1 for token in vocab.get_itos() if token in glove.stoi)\n",
    "print(f\"GloVe coverage: {known / len(vocab):.2%}\")\n",
    "\n",
    "# ========== Preprocessing ==========\n",
    "def preprocess(example):\n",
    "    tokens = tokenizer(example[\"text\"])\n",
    "    input_ids = vocab(tokens)[:MAX_LEN]\n",
    "    label = int(example[\"label\"])\n",
    "    return {\"input_ids\": input_ids, \"label\": label}\n",
    "\n",
    "train_data = train_data.map(preprocess)\n",
    "test_data = test_data.map(preprocess)\n",
    "train_data.set_format(type=\"python\", columns=[\"input_ids\", \"label\"])\n",
    "test_data.set_format(type=\"python\", columns=[\"input_ids\", \"label\"])\n",
    "\n",
    "# ========== Collate Function ==========\n",
    "def collate_batch(batch):\n",
    "    texts = [torch.tensor(sample[\"input_ids\"], dtype=torch.int64) for sample in batch]\n",
    "    labels = [torch.tensor(sample[\"label\"], dtype=torch.float32) for sample in batch]\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return texts_padded.to(device), torch.stack(labels).to(device)\n",
    "\n",
    "# ========== DataLoaders ==========\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=collate_batch, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=collate_batch, num_workers=0, pin_memory=True)\n",
    "\n",
    "# ========== Check Class Balance ==========\n",
    "print(\"Label distribution in training set:\")\n",
    "print(Counter([sample[\"label\"] for sample in train_data]))\n",
    "\n",
    "# ========== Build Embedding Matrix ==========\n",
    "embedding_matrix = torch.zeros(len(vocab), EMBEDDING_DIM)\n",
    "\n",
    "for idx, token in enumerate(vocab.get_itos()):\n",
    "    if token in glove.stoi:\n",
    "        embedding_matrix[idx] = glove[token]\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(EMBEDDING_DIM) * 0.6  # small random vector for unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerSentimentClassifier(\n",
       "  (embedding): Embedding(63925, 50, padding_idx=0)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crearte the Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class TransformerSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len, dropout=0.1):\n",
    "        super(TransformerSentimentClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=vocab[\"<pad>\"]) # embedding layer\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, max_len) # Positional Encoding\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True \n",
    "        ) # Transformer Encoder Layer\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers) # Transformer Encoder\n",
    "\n",
    "        self.classifier = nn.Sequential( # Final classification layer\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x == vocab[\"<pad>\"]).to(torch.bool)\n",
    "        \n",
    "        x = self.embedding(x)  # Embed tokens\n",
    "        x = self.positional_encoding(x) # Add positional encoding\n",
    "\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask) # Transformer Encoder\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "\n",
    "        return self.classifier(x).squeeze()\n",
    "    \n",
    "# Initialize model, loss function, and optimizer\n",
    "model = TransformerSentimentClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    num_heads=1,  \n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=1,\n",
    "    max_len=MAX_LEN\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd7739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 391/391 [01:17<00:00,  5.06it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 391/391 [01:12<00:00,  5.40it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 391/391 [01:20<00:00,  4.84it/s, loss=0.0703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 391/391 [01:19<00:00,  4.95it/s, loss=0.0298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.0298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 391/391 [01:23<00:00,  4.68it/s, loss=0.0174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for texts, labels in loop:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=total_loss / (loop.n + 1))\n",
    "    \n",
    "    # Step the scheduler once per epoch\n",
    "    scheduler.step(total_loss / len(train_loader))  # at end of each epoch\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba776c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8018\n",
      "Precision: 0.8152\n",
      "Recall:    0.7804\n",
      "F1 Score:  0.7974\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        predicted = (torch.sigmoid(outputs) >= 0.50).float()  # only apply sigmoid here\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Now compute metrics on all collected predictions\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe7d3e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predictions: (tensor([0., 1.]), tensor([13034, 11966]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique predictions:\", torch.unique(torch.tensor(y_pred), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b45145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 12500, 1: 12500})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([sample[\"label\"] for sample in test_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
